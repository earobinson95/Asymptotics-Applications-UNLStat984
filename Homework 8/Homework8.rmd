---
title: "Homework 8"
author: "Emily Robinson"
date: "November 14, 2019"
output:
  pdf_document:
      keep_tex:  true
subtitle: STAT 984
theme: cerulean
fontsize: 12pt
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
---

<style type="text/css">

h1.title {
  font-size: 18px;
  color: Black;
  text-align: center;
}
h3.subtitle{
  font-size: 12px;
  color: Black;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 12px;
  font-family: "Times New Roman", Times, serif;
  color: Black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 12px;
  font-family: "Times New Roman", Times, serif;
  color: Black;
  text-align: center;
}
</style>

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=5, fig.height=4, fig.align = "center")
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=50),tidy=TRUE)
```

### Exercise 6.1

For a given $n$, let $X_1,...,X_n$ be independent and identically distributed with distribution function
$$P(X_i\le t) = \frac{t^3+\theta^3}{2\theta^3} \text{ for } t\in[-\theta,\theta].$$
Let $X_{(1)}$ denote the first order statistic from the sample of size $n$; that is, $X_{(1)}$ is the smallest of the $X_i$.

(a) Prove that $-X_{(1)}$ is consistent for $\theta.$

Let $0 < \epsilon < 4\theta.$ Then
\begin{align*}
P(|-X_{(1)}-\theta|>\epsilon) & = P(-X_{(1)}-\theta \le -\epsilon)\\
& = P(X_{(1)}+\theta \ge \epsilon)\\
& = P(X_{(1)}\ge \epsilon - \theta)\\
& = [P(X \ge \epsilon - \theta)]^n\\
& = [1 - P(X \le \epsilon - \theta)]^n\\
& = \left[1 - \frac{(\epsilon - \theta)^3 + \theta^3}{2\theta^3}\right]^n\\
& \rightarrow 0 & \text{since}  \left[1 - \frac{(\epsilon - \theta)^3 + \theta^3}{2\theta^3}\right] \in (-1, 1).
\end{align*}

(b) Prove that
$$n(\theta+X_{(1)})\overset{d}\rightarrow Y,$$
      where $Y$ is a random variable with an exponential distribution. Find E$(Y)$ in terms of $\theta$.
      
Consider
\begin{align*}
P[n(\theta+X_{(1)})\ge y] & = P\left[\theta+X_{(1)}\ge \frac{y}{n}\right]\\
& = P\left[X_{(1)}\ge \frac{y}{n}-\theta\right]\\
& = \left(P\left[X \ge \frac{y-n\theta}{n}\right]\right)^n\\
& = \left(1 - P\left[X \le \frac{y-n\theta}{n}\right]\right)^n\\
& = \left(1 - \frac{\left(\frac{y-n\theta}{n}\right)^3+\theta^3}{2\theta^2}\right)^n\\
& = \left(1 - \frac{\left(y-n\theta\right)^3+(n\theta)^3}{n^32\theta^2}\right)^n\\
& \rightarrow e^{-\left(\frac{3y}{2\theta}\right)}\\
& = e^{-\frac{y}{\frac{2\theta}{3}}}.
\end{align*}
Therefore, $Y\overset{asy}\sim Exp\left(\frac{2\theta}{3}\right)$ and E$[Y]=\frac{2\theta}{3}.$

(c) For a fixed $\alpha$, define
$$\delta_{\alpha,n}=-\left(1+\frac{\alpha}{n}X_{(1)}\right).$$
    Find, with proof, $\alpha^*$ such that
$$n(\theta-\delta_{\alpha^*,n})\overset{d}\rightarrow Y - \text{E}(Y),$$
    where $Y$ is the same random variable as in part (b).
    
Consider

\begin{align*}
P\left(n(\theta - (1+\frac{\alpha^* }{n})X_{(1)}) > t \right) &= P\left(\theta - (1+\frac{\alpha^*}{n})X_{(1)} > t/n \right) \\
& = P\left(X_{(1)} > \frac{t/n-\theta}{(1+\frac{\alpha^*}{n})} \right)\\
& = \left[P\left(X > \frac{(t-n\theta)/n}{(n+\alpha^*)/n)} \right)\right]^n\\
& = \left[1 - P\left(X \le \frac{t-n\theta}{\alpha^*+n} \right)\right]^n\\
& = \left[1 - \frac{\left(\frac{t-n\theta}{\alpha^*+n}\right)^3+\theta^3}{2\theta^3}\right]^n\\
&\rightarrow e^{-3(\alpha^* \theta+t)/(2\theta)} \\
& = e^{-\frac{(t+\alpha^* \theta)}{\frac{2\theta}{3}}}.
\end{align*}

Then recall $Y\sim Exp(\frac{2\theta}{3}).$ Let $\alpha^* = \frac{2}{3},$ then
$$n(\theta-\delta_{\alpha^*,n})\overset{d}\rightarrow Y - \alpha^* \theta \overset{d}= Y - \frac{2\theta}{3}=Y-E[Y].$$

(d) Compare the two consistent $\theta$-estimators $\delta_{\alpha^*,n}$ and $-X_{(1)}$ empirically as follows. For $n\in\{10^2, 10^3, 10^4\}$, take $\theta = 1$ and simulate 1000 samples of size $n$ from the distribution of $X_i$. From these 1000 samples, estimate the bias and mean squared error of each estimator. Which of the two appears better? Do your empirical results agree with the theoretical results in parts (b) and (c)?

The $\delta_{\alpha^*,n}$ appears to be the better estimator due to it's low unbiasedness. These agree with the theoretical results above.  

```{r p6.1d, message=FALSE, warning=FALSE}
library(pracma)
estCompare1 <- function(n = 100, theta = 1, samples = 1000) {
  est <- matrix(NA, samples, 2)
  for(i in 1:samples){
    x1 <- min(theta*nthroot((2*runif(n)-1),3))
    est[i,1] <- x1
    est[i,2] <- -(1+2/(3*n))*x1
    bias <- est - theta
  }
  Bias <- colMeans((bias))
  Var <- apply(est, 2, var)
  MSE <- Var + Bias^2
  bias_mse <- cbind(Bias, MSE)
  rownames(bias_mse) <- c("X(1)", "Delta")
  round(bias_mse,10)
}
estCompare1(n = 100, theta = 1, samples = 1000)
estCompare1(n = 1000, theta = 1, samples = 1000)
estCompare1(n = 10000, theta = 1, samples = 1000)
```

### Exercise 6.2

Let $X_1, X_2,...$ be independent uniform $(0,\theta)$ random variables. Let $X_{(n)}=\max\{X_1,...,X_n\}$ and consider the three estimators
\begin{align*}
\delta_n^0=X_{(n)} & \delta_n^1=\frac{n}{n-1}X_{(n)} & \delta_n^2=\left(\frac{n}{n-1}\right)^2X_{(n)}
\end{align*}

(a) Prove that each estimator is consistent for $\theta.$

\begin{proof}
Let $0<\epsilon<\theta.$ Then
\begin{align*}
P(|X_{(n)}-\theta|>\epsilon) & = P(X_{(n)} - \theta \le -\epsilon)\\
& = P(X_{(n)} \le \theta - \epsilon)\\
& = [P(X \le \theta - \epsilon)]^n\\
& = [F_X(\theta-\epsilon)]^n\\
& = \left(\frac{\theta-\epsilon}{\theta}\right)^n\\
& \rightarrow 0. & (\text{since } \theta-\epsilon < 0)
\end{align*}
Therefore, $X_{(n)}\overset{P}\rightarrow\theta$ and $X_{(n)}$ is consistent for $\theta.$ Then since $\frac{n}{n-1}\rightarrow 1,$ we know $\delta_n^1\overset{P}\rightarrow\theta$ and $\delta_n^2\overset{P}\rightarrow\theta$.
\end{proof}

(b) Perform an empirical comparison of these three estimators for $n = 10^2, 10^3, 10^4.$ Use $\theta = 1$ and simulate 1000 samples of size $n$ from uniform $(0,1)$. From these 1000 samples, estimate the bias and mean squared error of each estimator. Which one of the three appears to be best?

Based on the results, it appears $\delta_n^1$ is the best in terms of MSE. It is obvious that its bias is much lower than that of the other estimators.

```{r p6.2}
estCompare <- function(n = 100, theta = 1, samples = 1000) {
  est <- matrix(NA, samples, 3)
  for(i in 1:samples){
    xn <- max(theta*runif(n))
    est[i,1] <- xn
    est[i,2] <-xn*(n/(n-1))
    est[i,3] <- xn*(n/(n-1))^2
    bias <- est - theta
  }
  Bias <- colMeans((bias))
  Var <- apply(est, 2, var)
  MSE <- Var + Bias^2
  bias_mse <- cbind(Bias, MSE)
  rownames(bias_mse) <- c("delta0", "delta1", "delta2")
  bias_mse
}
estCompare(n = 100, theta = 1, samples = 1000)
estCompare(n = 1000, theta = 1, samples = 1000)
estCompare(n = 10000, theta = 1, samples = 1000)
```

(c) Find the asymptotic distribution of $n(\theta-\delta_n^i)$ for $i=0,1,2$. Based on your results, which of the three appears to be the best estimator and why? (For the latter question, don't attempt to make a rigorous mathematical argument; simply give an educated guess.)

Consider
\begin{align*}
P[n(\theta-\delta_n^i)\ge t] & = P\left[n\left(\theta - X_{(n)}\left(\frac{n}{n-1}\right)^i\right)\ge t\right]\\
& = P\left[X_{(n)}\left(\frac{n}{n-1}\right)^i \le \theta - \frac{t}{n}\right]\\
& = P\left[X_{(n)}\le \left(\theta - \frac{t}{n}\right)\left(\frac{n-1}{n}\right)^i\right]\\
& = \left[P\left[X\le \left(\theta - \frac{t}{n}\right)\left(\frac{n-1}{n}\right)^i\right]\right]^n\\
& = \left[\left(\theta - \frac{t}{n}\right)\left(\frac{n-1}{n}\right)^i\right]^n\\
& = \left(\frac{n-1}{n}\right)^ni\left(\theta - \frac{t}{n}\right)^n\\
& \rightarrow e^{-1}e^{-t/\theta}\\
& = e^{-(t+\theta i)/\theta}.
\end{align*}
Then consider $T\sim \text{Exp}(\theta).$ Then
$$n(\theta-\delta_n^i)\overset{d}\rightarrow T-i\theta.$$
Therefore, when $i=1,$ the estimator is asymptotically unbiased. This agrees with our results in part (b).

### Exercise 6.5

Let $X_1,...,X_n$ be a simple random sample from the distribution function $F(x)=[1-(1/x)]I\{x>1\}$.

(a) Find the joint asymptotic distribution of $(X_{(n-1)}/n,X_{(n)/n}).$

    \textbf{Hint:} Proceed as in Example 6.5.
    
The inverse is $F^{-1}(u)=\frac{1}{1-u}.$ Let $U_1,...,U_n\overset{iid}\sim \text{Unif}(0,1).$ Then 
$$\begin{pmatrix} X_{(n-1)} \\ X_{(n)}\end{pmatrix} \overset{d}= \begin{pmatrix} \frac{1}{1-U_{(n-1)}} \\ \frac{1}{1-U{(n)}}\end{pmatrix}.$$
Then from Example 6.4, we know

$$\begin{pmatrix} n(1-U_{(n-1)}) \\ n(1-U{(n)})\end{pmatrix}\overset{d}\rightarrow \begin{pmatrix} Y_1+Y_2 \\ Y_1\end{pmatrix}$$
where $Y_1, Y_2 \overset{iid}\sim \text{Exp}(1).$ Therefore, by Slutksy's Theroem, 
$$\begin{pmatrix} \frac{X_{(n-1)}}{n} \\ \frac{X_{(n)}}{n}\end{pmatrix} \overset{d}= \begin{pmatrix} \frac{1}{n(1-U_{(n-1)})} \\ \frac{1}{n(1-U{(n)})}\end{pmatrix}\overset{d}\rightarrow \begin{pmatrix} \frac{1}{Y_1+Y_2} \\ \frac{1}{Y_1}\end{pmatrix}.$$
    
(b) Find the asymptotic distribution of $X_{(n-1)}/n,X_{(n)/n}$.

From part a, we know

$$\frac{X_{(n-1)}}{X_{(n)}}\overset{d}\rightarrow \frac{Y_1}{Y_1+Y_2}.$$
Then $\frac{Y_1}{Y_1+Y_2}\in(0,1).$ Then
\begin{align*}
P\left(\frac{Y_1}{Y_1+Y_2} \le t \right) & = E\left[P\left(\frac{Y_1}{Y_1+Y_2} \le t |Y_2 \right)\right]\\
& = E\left[P\left(Y_1 \le \frac{tY_2}{1-t} |Y_2 \right)\right]\\
& = E\left[1 -  e^{\frac{tY_2}{1-t}}\right]\\
& = 1 - \int_0^\infty e^{\frac{ty}{1-t}}e^{-y} dy\\
& = 1 - \int_0^\infty e^{-\frac{y}{1-t}} dy & \text{looks like Exp}(1-t)\\
& = 1 - (1-t)\\
& = t.
\end{align*}

### Exercise 6.8

Let $X_1,...,X_n$ be independent uniform$(0,2\theta)$ random variables.

(a) Let $M=(X_{(1)}+X_{(n)})/2.$ Find the asymptotic distribution of $n(M-\theta)$.

Recall from Example 6.3, we know

$$\begin{pmatrix}nU_{(1)} \\ n(1-U_{(n)}\end{pmatrix}\overset{d}\rightarrow \begin{pmatrix}Y_1 \\ Y_2\end{pmatrix}$$
where $U_i \overset{iid}\sim \text{Unif}(0,1)$ and $Y_1, Y_2 \overset{iid}\sim \text{Exp}(1).$

Therefore, 
$$\begin{pmatrix} \frac{n2\theta U_{(1)}}{2\theta} \\ \frac{2n(1-U_{(n)})}{2\theta}\end{pmatrix}\overset{d}\rightarrow \begin{pmatrix}Y_1 \\ Y_2\end{pmatrix} \implies \frac{n}{2\theta}\begin{pmatrix} X_{(1)} \\ 2\theta - X_{(n)}\end{pmatrix}\overset{d}\rightarrow \begin{pmatrix}Y_1 \\ Y_2\end{pmatrix}.$$

Thus,
\begin{align*}
n(M-\theta) & = n(\frac{X_{(1)}+X_{(n)}}{2}-\theta)\\
& = \frac{n}{2}(X_{(1)}+X_{(n)}-2\theta)\\
& = \frac{n}{2}\left(X_{(1)}- (2\theta - X_{(n)})\right)\\
&\overset{d}\rightarrow \theta(Y_1 - Y_2) \sim \text{Laplace}(0,\theta).
\end{align*}

Therefore, E$[\theta(Y_1 - Y_2)]=0$ and Var$[\theta(Y_1 - Y_2)]=2\theta^2.$

(b) Compare the asymptotic performance of the three estimators $M, \bar X_n,$ and the sample median $\tilde X_n$ by considering their relative efficiencies.

From part a, we know the asymptotic variance of $M\approx \frac{2\theta^2}{n^2}$. Then since Var$(X_i) = \frac{(2\theta)^2}{12},$ we know Var$(\bar X_n) = \frac{\theta^2}{3n}$. Then, by Theorem 3.7, we know
$$\sqrt{n}(\tilde X_n- \theta) \overset{d}\rightarrow N(0,\theta^2)$$
since $\frac{p(1-p)}{f(\xi)^2}=\frac{(1/2)(1/2)}{1/(2\theta)^2}=\frac{(2\theta)^2}{4}=\theta^2.$
Therefore, the asymptotic variance of $\tilde X_n \approx \frac{\theta^2}{n}.$

Then
\begin{align*}
e_{M,\bar X_n} & = \frac{Var(M)}{Var(\bar X_n)}= \frac{6}{n} \rightarrow 0\\
e_{M,\tilde X_n} & = \frac{Var(M)}{Var(\tilde X_n)}= \frac{2}{n} \rightarrow 0\\
e_{\bar X_n, \tilde X_n} & = \frac{Var(\bar X_n)}{Var(\tilde X_n)}= \frac{1}{3} \rightarrow \frac{1}{3}.
\end{align*}

(c) For $n\in \{101, 1001, 10001\}$, generate 500 samples of size $n$, taking $\theta = 1.$ Keep track of $M,\bar X_n$, and $\tilde X_n$ for each sample. Construct a $3 \times 3$ table in which you report the sample variance of each estimator for each value of $n$. Do your simulation results agree with your theoretical results in part (b)?

```{r p6.8}
f <- function(n) {
  x <- 2*runif(n)
  c(M=(min(x)+max(x))/2, Xbar=mean(x), Xtilde=median(x))
}
rbind(n101 = apply(replicate(500, f(101)), 1, var),
      n1001 = apply(replicate(500, f(1001)), 1, var),
      n10001 = apply(replicate(500, f(10001)), 1, var))
```

### Exercise 6.12
Let $X_1,...,X_n$ be a random sample from Uniform$(0,2\theta)$. Find the asymptotic distributions of the median, the midquartile range, and $\frac{2}{3}Q_3$, where $Q_3$ denotes the third quartile and the midquartile range is the mean of the 1st and 3rd quartiles. Compare these three estimates of $\theta$ based on their asymptotic variances.

Consider the median, $\tilde X_n.$ Then $p=\frac{1}{2}.$ Therefore, by Theorem 6.7, $\frac{p(1-p)}{F'(\xi_p)^2}=\frac{1/2(1-1/2)}{(1/2\theta)^2}=\theta^2$ implies
\begin{align*}
&&\sqrt{n}(\tilde X_n - \Xi_p)\overset{d}\rightarrow N(0,\theta^2)\\
&\implies& \sqrt{n}(\tilde X_n-\theta)\overset{d}\rightarrow N(0,\theta^2)\\
&\implies& \tilde X_n \overset{d}\rightarrow N(\theta, \frac{\theta^2}{n}).
\end{align*}

Consider the mid-quartile range, let $p_1=1/4$ and $p_2 = 3/4.$ Then by Theorem 6.7,
$$\sqrt{n}\left[\begin{pmatrix}Q_{[1/4]} \\ Q_{[3/4]}\end{pmatrix} - \begin{pmatrix} \xi_{[1/4]} \\ \xi_{[3/4]}\end{pmatrix}\right]\overset{d}\rightarrow N_2\left(\boldsymbol 0, \Sigma \right)$$

where
$$\Sigma = 2\theta^2 \begin{pmatrix} p_1(1-p_1) & p_1(1-p_2) \\ p_1(1-p_2) &p_2(1-p_2) \end{pmatrix}= \begin{pmatrix} \frac{3\theta^2}{4} & \frac{\theta^2}{4} \\ \frac{\theta^2}{4} & \frac{3\theta^2}{4}\end{pmatrix}.$$ Then consider the mid-quartile range, $g(Q_{[1/4]}, Q_{[3/4]})=\frac{Q_{[1/4]}+ Q_{[3/4]}}{2}$. Then $A = [\triangledown g(Q_{[1/4]}, Q_{[3/4]})]^T=(1/2 , 1/2)^T.$ Therefore, $A\Sigma A^T = \frac{\theta^2}{2}$. Thus,
$$\sqrt{n}\left(\frac{Q_{[1/4]}+Q_{[3/4]}}{2}-\theta\right)\overset{d}\rightarrow N\left(0, \frac{\theta^2}{2}\right).$$

Consider $\frac{2}{3}Q_3$. Then $p = 3/4.$ Therefore, by Theorem 6.7, $\frac{p(1-p)}{F'(\xi_p)^2}=\frac{3/4(1-3/4)}{(1/2\theta)^2}=\frac{3\theta^2}{4}$ implies
\begin{align*}
&&\sqrt{n}(Q_{[3/4]} - \Xi_p)\overset{d}\rightarrow N(0,\frac{3\theta^2}{4})\\
&\implies& \sqrt{n}(Q_{[3/4]}-\theta)\overset{d}\rightarrow N(0,\frac{3\theta^2}{4})\\
&\implies& Q_{[3/4]} \overset{d}\rightarrow N(\theta, \frac{3\theta^2}{4})\\
&\implies& \frac{2}{3}Q_{[3/4]} \overset{d}\rightarrow N\left(\theta, \left(\frac{2}{3}\right)^2\frac{3\theta^2}{4}\right)\\
&\implies& \frac{2}{3}Q_{[3/4]} \overset{d}\rightarrow N\left(\theta, \frac{\theta^2}{3n}\right).
\end{align*}
