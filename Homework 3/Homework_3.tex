\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Homework 3},
            pdfauthor={Emily Robinson},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Homework 3}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \subtitle{STAT 984}
  \author{Emily Robinson}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{September 26, 2019}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\begin{document}
\maketitle

\hypertarget{exercise-1.38}{%
\subsubsection{Exercise 1.38}\label{exercise-1.38}}

Let \(f(x)\) be a convex function on some interval, and let \(x_0\) be
any point on the interior of that interval.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Prove that \begin{align*}
  &&\lim_{x\rightarrow x_0+} \frac{f(x)-f(x_0)}{x-x_0} && (1.38)
  \end{align*} exists and is finite, that is, a one-sided derivative
  exists at \(x_0\).
\end{enumerate}

Hint: Using Definition 1.30, show that the fraction in expression (1.38)
is non-increasing and bounded below as \(x\) decreases to \(x_0.\)

\begin{proof}
Consider $f(x)$ on convex on $[a,b]$ where $a < x_0 < b.$ Let there be $x_2 \in (x_0, b).$ Then $x_2 = \alpha x_0 + (1-\alpha) b$ and $f(x_2) = f(\alpha x_0 + (1-\alpha) b) \le \alpha f(x_0) + (1-\alpha)f(b).$ Therefore, 
$$\frac{f(x_2)-f(x_0)}{x_2-x_0}\le\frac{\alpha f(x_0)+(1-\alpha)f(b)-f(x_0)}{\alpha x_0+(1-\alpha)b - x_0} = \frac{f(b) -f(x_0)}{b - x_0}.$$
Now let there be $x_1 \in (x_0, x_2).$ Then $x_1 = \alpha x_0 + (1-\alpha) x_2$ and $f(x_1) = f(\alpha x_0 + (1-\alpha) x_2) \le \alpha f(x_0) + (1-\alpha)f(x_2).$ Therefore, 
$$\frac{f(x_1)-f(x_0)}{x_1-x_0}\le\frac{\alpha f(x_0)+(1-\alpha)f(x_2)-f(x_0)}{\alpha x_0+(1-\alpha)x_2 - x_0} = \frac{f(x_2) -f(x_0)}{x_2 - x_0}.$$
Thus, for $x_0 < x_1 < x_2 < b,$
$$\frac{f(x_1)-f(x_0)}{x_1-x_0} \le \frac{f(x_2) -f(x_0)}{x_2 - x_0} \le \frac{f(b) -f(x_0)}{b - x_0}.$$
Therefore, $\frac{f(x) - f(x_0)}{x-x_0}$ is non-increasing as $x \rightarrow x_0+$ and $f$ has a right handed derivative $f'(x_0+)$.

Now consider $x_0 \in (a, x_1).$ Then $x_0 = \alpha a + (1-\alpha) x_1$ and $f(x_0) = f(\alpha a + (1-\alpha) x_1) \le \alpha f(a) + (1-\alpha)f(x_1).$ Therefore,
$$\frac{f(x_1) -f(x_0)}{x_1 - x_0}\ge \frac{f(x_1)-f(a)}{x_1-a}.$$
Therefore, $\frac{f(x) - f(x_0)}{x-x_0}$ is bounded below.

Thus, since $$\frac{f(x_1)-f(a)}{x_1-a}\le \frac{f(x_1) -f(x_0)}{x_1 - x_0} \le\frac{f(b) -f(x_0)}{b - x_0},$$ $\frac{f(x) - f(x_0)}{x-x_0}$ is finite and exists. 
\end{proof}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Prove that there exists a linear function \(g(x) = ax+b\) such that
  \(g(x_0) = f(x_0)\) and \(g(x) \le f(x)\) for all \(x\) in the
  interval. This fact is the supporting hyperplane property in the case
  of a convex function taking a real argument.
\end{enumerate}

Hint: Let \(f'(x_0+)\) denote the one-sided derivative of part (a).
Consider the line \(f(x_0)+f'(x_0+)(x-x_0).\)

\begin{proof}
Using part a, consider $g(x) = f(x_0)+f'(x_0+)(x-x_0)$. Then
$$g(x) = f(x_0)+f'(x_0+)(x-x_0) \le f(x_0)+\frac{f(x)-f(x_0)}{x-x_0}(x-x_0)=f(x).$$
Therefore, $g(x) \le f(x).$ Then
$$g(x_0) = f(x_0) + f'(x_0+)(x_0-x_0) = f(x_0).$$
Therefore, $g(x_0) = f(x_0)$.
\end{proof}

\hypertarget{exercise-1.39}{%
\subsubsection{Exercise 1.39}\label{exercise-1.39}}

Prove Holder's inequality: For random variables \(X\) and \(Y\) and
positive \(p\) and \(a\) such that \(p+q=1\), \begin{align*}
&& E|XY|\le (E|X|^{1/p})^p(E|Y|^{1/q})^q. && (1.39)
\end{align*} (If \(p=q=1/2\), inequality 1.39 is also called the
Cauchy-Schwartz inequality.)

Hint: Use the convexity of \(\exp(x)\) to prove that
\(|abXY|\le p|aX|^{1/p}+q|bY|^{1/q}\) whenever \(aX\ne 0\) and
\(bY \ne 0\) (the same inequality is also true if \(aX=0\) or \(bY=0\)).
Take expectations, then find values for the scalars \(a\) and \(b\) that
give the desired result when the right side of inequality (1.39) is
nonzero.

\begin{proof}
Consider $f(x) = e^x$ convex. Let $\alpha = p$, thus $1-\alpha = q.$ Then by the definition of convex, 
$$f(\alpha A +(1-\alpha)B) = f(pA+qB)\le pf(A)+qf(B).$$
Now let $A = \frac{\log|aX|}{p}$ and $B = \frac{\log|bY|}{q}.$ Then
\begin{align*}
&& f\left(p\frac{\log|aX|}{p}+q\frac{\log|bY|}{q}\right) & \le pf\left(\frac{\log|aX|}{p}\right)+qf\left(\frac{\log|bY|}{q}\right)\\
& \implies & f(\log|abXY|) & \le pe^{\log(|aX|^{1/p})}+qe^{\log|bY|^{1/q}}\\
& \implies & e^{\log|abXY|} & \le p|aX|^{1/p}+q|bY|^{1/q}\\
& \implies & |abXY| & \le p|aX|^{1/p}+q|bY|^{1/q}.
\end{align*}
Now let $a = (E|X|^{1/p})^{-p}$ and $b = (E|Y|^{1/q})^{-q}.$ Then
\begin{align*}
&& E|abXY| & \le E\left[p|aX|^{1/p}+q|bY|^{1/q}\right]\\
&\implies & |ab|E|XY| & \le p|a|^{1/p}E|X|^{1/p}+q|b|^{1/q}E|Y|^{1/q}\\
& \implies & (E|X|^{1/p})^{-p}(E|Y|^{1/q})^{-q}E|XY| & \le p\left[(E|X|^{1/p})^{-p}\right]^{1/p}E|X|^{1/p} + q\left[(E|Y|^{1/q})^{-q}\right]^{1/q}E|Y|^{1/q}\\
& \implies & \frac{E|XY|}{(E|X|^{1/p})^{p}(E|Y|^{1/q})^{q}} & \le p\frac{E|X|^{1/p}}{E|X|^{1/p}} + q\frac{E|Y|^{1/q}}{E|Y|^{1/q}}\\
& \implies & \frac{E|XY|}{(E|X|^{1/p})^{p}(E|Y|^{1/q})^{q}} & \le p+q\\
& \implies &\frac{E|XY|}{(E|X|^{1/p})^{p}(E|Y|^{1/q})^{q}} & \le 1\\
& \implies & E|XY| & \le (E|X|^{1/p})^{p}(E|Y|^{1/q})^{q}.
\end{align*}
\end{proof}

\hypertarget{exercise-1.40}{%
\subsubsection{Exercise 1.40}\label{exercise-1.40}}

Use Holder's Inequality (1.39) to prove that if \(\alpha > 1,\) then
\[(E|X|)^\alpha\le E|X|^\alpha.\]

Hint: Take \(Y\) to be a constant in Inequality (1.39).

\begin{proof}
Let $Y = c,$ constant. Then by Holder's Inequality, 
\begin{align*}
&& E|XY| & \le (E|X|^{1/p})^p(E|Y|^{1/q})^q\\
& \implies & |c| E|X| & \le (E|X|^{1/p})^p(|c|^{1/q})^q\\
& \implies & E|X| & \le (E|X|^{1/p})^p\\
& \implies & (E|X|)^{1/p} & \le E|X|^{1/p}.
\end{align*}
Then let $\alpha = 1/p.$ Since $p + q = 1.$ Then $p < 1$ implies $1/p > 1.$ Thus, $\alpha > 1.$ Therefore, $(E|X|)^{\alpha} \le E|X|^{\alpha}.$
\end{proof}

\hypertarget{exercise-1.45}{%
\subsubsection{Exercise 1.45}\label{exercise-1.45}}

For any nonnegative random variable \(Y\) with finite expectation, prove
that \begin{align*}
&& \sum_{i=1}^{\infty} P(Y\ge i) \le EY. && (1.43)
\end{align*}

Hint: First, prove that equality holds if \(Y\) is supported on the
nonnegative integers. Then note for a general \(Y\) that
\(E{\left\lfloor Y \right\rfloor} \le EY,\) where
\({\left\lfloor x \right\rfloor}\) denotes the greatest integer less
than or equal to \(x.\)

Though we will not do so here, it is possible to prove a statement
stronger than inequality (1.43) for nonnegative random variables,
namely, \[\int_0^\infty P(Y\ge t)dt = EY.\] (This equation remains true
if \(EY = \infty\).) To sketch a proof, note that if we can prove
\(\int E f(Y,t)dt = E\int f(Y,t)dt\), the result follows immediately by
taking \(f(Y,t) = I\{Y\ge t\}.\)

\begin{proof}
Assume $Y$ is supported on the non-negative integers. Then 
\begin{align*}
\sum_{i =1}^{\infty}P(Y\ge i) & = [P(Y=1)+P(Y=2)+\cdot \cdot \cdot]\\
& +[P(Y=2)+P(Y=3)+\cdot \cdot \cdot]\\
& + [P(Y=3)+P(Y=4)+\cdot \cdot \cdot] + \cdot \cdot \cdot\\
& = 1\cdot P(Y = 1) + 2\cdot P(Y=2) + 3\cdot P(Y=3) + \cdot \cdot \cdot\\
& = \sum_{i=1}^\infty i P(Y = i)\\
& = \sum_{i=1}^\infty y P(Y = y)\\
& = E[Y].
\end{align*}
Then note, for a general non-negative $Y$, $E{\left\lfloor Y \right\rfloor} \le EY.$ Therefore, $\sum_{i=1}^{\infty} P(Y\ge i) \le EY.$ 

\end{proof}

\hypertarget{exercise-2.1}{%
\subsubsection{Exercise 2.1}\label{exercise-2.1}}

For each of the three cases below, prove that
\(X_n\overset{P}\rightarrow 1:\)

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  \(X_n=1+nY_n,\) where \(Y_n\) is a Bernoulli random variable with mean
  \(1/n.\)
\end{enumerate}

\begin{proof}
Let $\epsilon>0$. Then 
\begin{align*}
P(|X_n -1|<\epsilon) & = P(|1+nY_n-1|<\epsilon)\\
& = P(|nY_n|<\epsilon)\\
& = P(Y_n < \epsilon/n)\\
& = 1 - 1/n\\
& \rightarrow 1.
\end{align*}
Thus, $X_n\overset{P}\rightarrow 1.$
\end{proof}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \(X_n = Y_n/\log n\), where \(Y_n\) is a Poisson random variable with
  mean \(\sum_{i=1}^n(1/i)\).

  \begin{proof}
  Consider $E(X_n) = \frac{\sum_{i=1}^n(1/i)}{\log n} \rightarrow 1$ and $Var(X_n) = \frac{\sum_{i=1}^n(1/i)}{(\log n)^2}\rightarrow 0.$ Thus, $X_n$ is asymptotically unbiased. Then by Chebyshev's Inequality,
  $$P(|X_n-1|\ge\epsilon) = P(|X_n-E[X_n]|\ge\epsilon) \le \frac{Var(X_n)}{\epsilon^2}\rightarrow 0.$$ Therefore, $P(|X_n-1|<\epsilon) \rightarrow 1$ and thus, $X_n\overset{P}\rightarrow 1.$
  \end{proof}
\item
  \(X_n = \frac{1}{n}\sum_{i=1}^n Y_i^2,\) where the \(Y_i\) are
  independent standard normal random variables.

  \begin{proof}
  Let $X_n = \frac{1}{n}\sum_{i=1}^n Y_i^2.$ Then $X_n \sim  Gamma(n, 1/n).$ Then $E(X_n) = 1$ and $Var(X_n) \rightarrow 0.$ Thus, $X_n$ is unbiased. Then by Chebyshev's Inequality,
  $$P(|X_n-1|\ge\epsilon) = P(|X_n-E[X_n]|\ge\epsilon) \le \frac{Var(X_n)}{\epsilon^2}\rightarrow 0.$$ Therefore, $P(|X_n-1|<\epsilon) \rightarrow 1$ and thus, $X_n\overset{P}\rightarrow 1.$
  \end{proof}
\end{enumerate}

\hypertarget{exercise-2.2}{%
\subsubsection{Exercise 2.2}\label{exercise-2.2}}

This exercise deals with bounded in probability sequences; see
Definition 2.6.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Prove that if \(X_n \overset{d}\rightarrow X\) for some random
  variable \(X\), then \(X_n\) is bounded in probability.
\end{enumerate}

Hint: You may use the fact that any interval of real numbers must
contain a point of continuity of \(F(x)\). Also, recall that
\(F(x)\rightarrow 1\) as \(x\rightarrow \infty.\)

\begin{proof}
Let $\epsilon > 0.$ Then since any interval of real numbers must contain a point of continuity of $F(x)$, there exists $a$ such that $F(x) < \epsilon/4$ and $b$ such that $F(b) > 1-\epsilon/4.$ Then by Definition 2.32, since $X_n \rightarrow X,$ we know $F_n(a)\rightarrow F(a)$ and $F_n(b)\rightarrow F(b).$ Then by definition of convergence, there exists an $N_1$ such that for all $n > N_1$, $|F_n(a)-F(a)| < \epsilon/4.$ Therefore, since $Fn(a)>F(a),$ we know $F_n(a) < \epsilon/2.$ Then similarly, there exists an $N_2$ such that for all $n>N_2$, $|F_n(b) -F(b)|<\epsilon/4.$ Therefore, since $F_n(b) < F(b),$ we know $F_n(b) > 1-\epsilon/2.$ Let $M = \max\{|a|,|b|\}$ and $N = \max\{N_1, N_2\}$. Then $P(|X_n|\le M) \ge P(|a|\le X_n \le b) = F_n(b) - F_n(a) > 1-\epsilon.$ Therefore, $X_n$ is bounded in probability.
\end{proof}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Prove that if \(X_n\) is bounded in probability and
  \(Y_n \overset{P}\rightarrow 0\), then
  \(X_nY_n\overset{P}\rightarrow 0.\)
\end{enumerate}

Hint: For fixed \(\epsilon > 0,\) argue that there must be \(M\) and
\(N\) such that \(P(|X_n|<M)>1-\epsilon/2\) and
\(P(|Y_n|<\epsilon/M)>1-\epsilon/2\) for all \(n>N.\) What is then the
smallest possible value of \(P(|X_n|<M\) and \(|Y_n|<\epsilon/M)\)? Use
this result to prove \(X_nY_n\overset{P}\rightarrow 0.\)

\begin{proof}
Let $\epsilon > 0.$ Then there exists $M$ and $N$ such that $P(|X_n| < M) > 1-\epsilon/2$ and $P(|Y_n|<\epsilon/M)>1-\epsilon/2$ for all $n>N.$ Then
\begin{align*}
P(|X_nY_n|<\epsilon) & \ge P(|X_n|<M \cap |Y_n| < \epsilon/M)\\
& = P(|X_n|<M)+P(|Y_n|<\epsilon/M) - P(|X_n|<M \cup |Y_n| < \epsilon/M)\\
& \ge P(|X_n|<M)+P(|Y_n|<\epsilon/M) - 1\\
& > (1-\epsilon/2) + (1-\epsilon/2) -1\\
& = 1-\epsilon.
\end{align*}
Therefore, $X_nY_n\overset{P}\rightarrow 0.$
\end{proof}

\hypertarget{exercise-2.4}{%
\subsubsection{Exercise 2.4}\label{exercise-2.4}}

Suppose that \(X_1,...X_n\) are independent and identically distributed
Uniform(0,1) random variables. For a real number \(t\), let
\[G_n(t) = \sum_{i=1}^nI\{X_i\le t\}.\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  What is the distribution of \(G_n(t)\) if \(0<t<1\)?
\end{enumerate}

Consider \(I(X_i \le t)\sim Bern(t)\) for \(0<t<1.\) Then,
\$\(G_n(t) = \sum_{i=1}^nI\{X_i\le t\}\sim Bin(n, t).\)

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Suppose \(c>0.\) Find the distribution of a random variable \(X\) such
  that \(G_n(c/n)\overset{d}\rightarrow X\). Justify your answer.
\end{enumerate}

Consider \(Y_n\sim Bin(n, p_n)\) where \(p_n \rightarrow c.\) Then for
\(y = 0, 1, 2, ...\), \begin{align*}
\lim_{n\rightarrow \infty}f_n(y) & = \lim_{n\rightarrow \infty} {n \choose y} p_n^y(1-p_n)^{n-y}\\
& = \lim_{n\rightarrow \infty} \frac{n(n-1(n-2)\cdot \cdot \cdot (n-y+1)}{y!} \left(\frac{c}{n}\right)^y\left(1-\frac{c}{n}\right)^{n-y}\\
& = \frac{n}{n}\cdot \frac{n-1}{n}\cdot \cdot \cdot \frac{n-y+1}{n}\cdot \frac{c^y}{y!}\left(1-\frac{c}{n}\right)^n\left(1-\frac{c}{n}\right)^{-y}\\
& = \frac{c^y e^{-c}}{y!}.
\end{align*} Therefore, \(Y_n \overset{d}\rightarrow Poisson(c).\)

Let \(Y_n = G_n(c/n)\sim Bin(n, c/n).\) Then
\(n\cdot \frac{c}{n} \rightarrow c.\) Thus, using the result from above,
\(Y_n\overset{d}\rightarrow Poisson(c).\)

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  How does your answer to part (b) change if \(X_1,...,X_n\) are from a
  standard exponential distribution instead of a uniform distribution?
  The standard exponential distribution function is \(F(t) = 1-e^{-t}.\)
\end{enumerate}

We now have \(I(X_i \le t)\sim Bern(1-e^{-t})\) for \(0<t<1.\) Then,
\(G_n(t) = \sum_{i=1}^nI\{X_i\le t\}\sim Bin(n, 1-e^{-t}).\) Thus, let
\(Y_n = G_n(c/n)\sim Bin(n, 1-e^{-c/n}).\) Then using Taylors series
expansion
\[n(1-e^{-c/n})=n\left(1 - \left(1 - \frac{c}{n} + \frac{c^2}{2n^2}\right)\right) = c + o\left(\frac{1}{n}\right).\]
Thus, using the result from above,
\(Y_n\overset{d}\rightarrow Poisson(c).\)


\end{document}
